{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"./\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"lightning_logs/fake_original\")\n",
    "writer_real = SummaryWriter(f\"lightning_logs/real_original\")\n",
    "step = 0\n",
    "BCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 0/1875                       Loss D: 1.4319, loss G: 0.6664\n",
      "Epoch [0/100] Batch 1874/1875                       Loss D: 0.9079, loss G: 1.1950\n",
      "Epoch [1/100] Batch 0/1875                       Loss D: 0.7784, loss G: 1.4337\n",
      "Epoch [1/100] Batch 1874/1875                       Loss D: 0.9280, loss G: 1.2885\n",
      "Epoch [2/100] Batch 0/1875                       Loss D: 0.7652, loss G: 1.2417\n",
      "Epoch [2/100] Batch 1874/1875                       Loss D: 1.1504, loss G: 1.0705\n",
      "Epoch [3/100] Batch 0/1875                       Loss D: 1.1354, loss G: 0.8769\n",
      "Epoch [3/100] Batch 1874/1875                       Loss D: 1.2305, loss G: 1.0136\n",
      "Epoch [4/100] Batch 0/1875                       Loss D: 0.8879, loss G: 1.3570\n",
      "Epoch [4/100] Batch 1874/1875                       Loss D: 1.2678, loss G: 1.0667\n",
      "Epoch [5/100] Batch 0/1875                       Loss D: 0.8079, loss G: 1.2948\n",
      "Epoch [5/100] Batch 1874/1875                       Loss D: 1.3231, loss G: 1.0117\n",
      "Epoch [6/100] Batch 0/1875                       Loss D: 1.5919, loss G: 0.8376\n",
      "Epoch [6/100] Batch 1874/1875                       Loss D: 1.2370, loss G: 1.1809\n",
      "Epoch [7/100] Batch 0/1875                       Loss D: 1.7976, loss G: 0.7270\n",
      "Epoch [7/100] Batch 1874/1875                       Loss D: 1.1770, loss G: 1.1841\n",
      "Epoch [8/100] Batch 0/1875                       Loss D: 1.0170, loss G: 1.1461\n",
      "Epoch [8/100] Batch 1874/1875                       Loss D: 1.1449, loss G: 1.2531\n",
      "Epoch [9/100] Batch 0/1875                       Loss D: 0.9795, loss G: 1.2154\n",
      "Epoch [9/100] Batch 1874/1875                       Loss D: 0.9719, loss G: 1.4079\n",
      "Epoch [10/100] Batch 0/1875                       Loss D: 1.2337, loss G: 0.9529\n",
      "Epoch [10/100] Batch 1874/1875                       Loss D: 1.0946, loss G: 1.2754\n",
      "Epoch [11/100] Batch 0/1875                       Loss D: 1.2469, loss G: 1.2759\n",
      "Epoch [11/100] Batch 1874/1875                       Loss D: 1.1220, loss G: 1.3596\n",
      "Epoch [12/100] Batch 0/1875                       Loss D: 0.6361, loss G: 1.8792\n",
      "Epoch [12/100] Batch 1874/1875                       Loss D: 1.1084, loss G: 1.4361\n",
      "Epoch [13/100] Batch 0/1875                       Loss D: 1.2346, loss G: 1.1102\n",
      "Epoch [13/100] Batch 1874/1875                       Loss D: 1.1611, loss G: 1.3037\n",
      "Epoch [14/100] Batch 0/1875                       Loss D: 1.2455, loss G: 0.9171\n",
      "Epoch [14/100] Batch 1874/1875                       Loss D: 1.2874, loss G: 1.2777\n",
      "Epoch [15/100] Batch 0/1875                       Loss D: 1.5009, loss G: 0.9851\n",
      "Epoch [15/100] Batch 1874/1875                       Loss D: 1.1722, loss G: 1.3338\n",
      "Epoch [16/100] Batch 0/1875                       Loss D: 1.1814, loss G: 1.4191\n",
      "Epoch [16/100] Batch 1874/1875                       Loss D: 1.0878, loss G: 1.4369\n",
      "Epoch [17/100] Batch 0/1875                       Loss D: 0.9790, loss G: 1.0341\n",
      "Epoch [17/100] Batch 1874/1875                       Loss D: 1.2664, loss G: 1.2402\n",
      "Epoch [18/100] Batch 0/1875                       Loss D: 1.0319, loss G: 1.8254\n",
      "Epoch [18/100] Batch 1874/1875                       Loss D: 1.0611, loss G: 1.4811\n",
      "Epoch [19/100] Batch 0/1875                       Loss D: 1.5946, loss G: 1.0332\n",
      "Epoch [19/100] Batch 1874/1875                       Loss D: 1.1463, loss G: 1.4534\n",
      "Epoch [20/100] Batch 0/1875                       Loss D: 1.4209, loss G: 1.0458\n",
      "Epoch [20/100] Batch 1874/1875                       Loss D: 1.1957, loss G: 1.3255\n",
      "Epoch [21/100] Batch 0/1875                       Loss D: 1.8520, loss G: 0.7034\n",
      "Epoch [21/100] Batch 1874/1875                       Loss D: 1.2609, loss G: 1.3605\n",
      "Epoch [22/100] Batch 0/1875                       Loss D: 1.3138, loss G: 1.1609\n",
      "Epoch [22/100] Batch 1874/1875                       Loss D: 1.2826, loss G: 1.2933\n",
      "Epoch [23/100] Batch 0/1875                       Loss D: 1.1649, loss G: 1.4914\n",
      "Epoch [23/100] Batch 1874/1875                       Loss D: 1.2085, loss G: 1.3587\n",
      "Epoch [24/100] Batch 0/1875                       Loss D: 1.1981, loss G: 1.2476\n",
      "Epoch [24/100] Batch 1874/1875                       Loss D: 1.0654, loss G: 1.4627\n",
      "Epoch [25/100] Batch 0/1875                       Loss D: 0.9835, loss G: 1.5278\n",
      "Epoch [25/100] Batch 1874/1875                       Loss D: 1.0534, loss G: 1.4784\n",
      "Epoch [26/100] Batch 0/1875                       Loss D: 0.8950, loss G: 1.5365\n",
      "Epoch [26/100] Batch 1874/1875                       Loss D: 1.0774, loss G: 1.4724\n",
      "Epoch [27/100] Batch 0/1875                       Loss D: 1.5519, loss G: 1.3863\n",
      "Epoch [27/100] Batch 1874/1875                       Loss D: 1.1135, loss G: 1.5310\n",
      "Epoch [28/100] Batch 0/1875                       Loss D: 1.1555, loss G: 1.3911\n",
      "Epoch [28/100] Batch 1874/1875                       Loss D: 1.1527, loss G: 1.4685\n",
      "Epoch [29/100] Batch 0/1875                       Loss D: 0.7339, loss G: 1.6630\n",
      "Epoch [29/100] Batch 1874/1875                       Loss D: 1.0838, loss G: 1.5900\n",
      "Epoch [30/100] Batch 0/1875                       Loss D: 1.1853, loss G: 1.4728\n",
      "Epoch [30/100] Batch 1874/1875                       Loss D: 1.1367, loss G: 1.4758\n",
      "Epoch [31/100] Batch 0/1875                       Loss D: 1.2247, loss G: 1.1473\n",
      "Epoch [31/100] Batch 1874/1875                       Loss D: 1.1744, loss G: 1.4137\n",
      "Epoch [32/100] Batch 0/1875                       Loss D: 0.8019, loss G: 1.5834\n",
      "Epoch [32/100] Batch 1874/1875                       Loss D: 0.9884, loss G: 1.6318\n",
      "Epoch [33/100] Batch 0/1875                       Loss D: 0.6118, loss G: 2.6636\n",
      "Epoch [33/100] Batch 1874/1875                       Loss D: 1.0793, loss G: 1.5332\n",
      "Epoch [34/100] Batch 0/1875                       Loss D: 1.1927, loss G: 1.4641\n",
      "Epoch [34/100] Batch 1874/1875                       Loss D: 1.0929, loss G: 1.4812\n",
      "Epoch [35/100] Batch 0/1875                       Loss D: 0.8831, loss G: 1.4489\n",
      "Epoch [35/100] Batch 1874/1875                       Loss D: 1.1360, loss G: 1.4790\n",
      "Epoch [36/100] Batch 0/1875                       Loss D: 0.8727, loss G: 1.6602\n",
      "Epoch [36/100] Batch 1874/1875                       Loss D: 1.1269, loss G: 1.4708\n",
      "Epoch [37/100] Batch 0/1875                       Loss D: 1.6080, loss G: 0.9282\n",
      "Epoch [37/100] Batch 1874/1875                       Loss D: 1.1908, loss G: 1.3365\n",
      "Epoch [38/100] Batch 0/1875                       Loss D: 1.3020, loss G: 1.1362\n",
      "Epoch [38/100] Batch 1874/1875                       Loss D: 1.1805, loss G: 1.3420\n",
      "Epoch [39/100] Batch 0/1875                       Loss D: 1.3340, loss G: 1.2631\n",
      "Epoch [39/100] Batch 1874/1875                       Loss D: 1.1881, loss G: 1.3244\n",
      "Epoch [40/100] Batch 0/1875                       Loss D: 1.4658, loss G: 1.0609\n",
      "Epoch [40/100] Batch 1874/1875                       Loss D: 1.1950, loss G: 1.2645\n",
      "Epoch [41/100] Batch 0/1875                       Loss D: 1.1370, loss G: 1.5225\n",
      "Epoch [41/100] Batch 1874/1875                       Loss D: 1.1802, loss G: 1.3023\n",
      "Epoch [42/100] Batch 0/1875                       Loss D: 1.3559, loss G: 1.1508\n",
      "Epoch [42/100] Batch 1874/1875                       Loss D: 1.1609, loss G: 1.3180\n",
      "Epoch [43/100] Batch 0/1875                       Loss D: 1.2732, loss G: 1.1660\n",
      "Epoch [43/100] Batch 1874/1875                       Loss D: 1.1717, loss G: 1.2581\n",
      "Epoch [44/100] Batch 0/1875                       Loss D: 1.2901, loss G: 1.0320\n",
      "Epoch [44/100] Batch 1874/1875                       Loss D: 1.2214, loss G: 1.2159\n",
      "Epoch [45/100] Batch 0/1875                       Loss D: 1.0657, loss G: 1.1099\n",
      "Epoch [45/100] Batch 1874/1875                       Loss D: 1.2259, loss G: 1.1730\n",
      "Epoch [46/100] Batch 0/1875                       Loss D: 1.1832, loss G: 1.0118\n",
      "Epoch [46/100] Batch 1874/1875                       Loss D: 1.2394, loss G: 1.1475\n",
      "Epoch [47/100] Batch 0/1875                       Loss D: 1.1378, loss G: 0.9200\n",
      "Epoch [47/100] Batch 1874/1875                       Loss D: 1.2036, loss G: 1.1955\n",
      "Epoch [48/100] Batch 0/1875                       Loss D: 1.1073, loss G: 1.1723\n",
      "Epoch [48/100] Batch 1874/1875                       Loss D: 1.2402, loss G: 1.1316\n",
      "Epoch [49/100] Batch 0/1875                       Loss D: 1.2896, loss G: 1.1323\n",
      "Epoch [49/100] Batch 1874/1875                       Loss D: 1.2329, loss G: 1.1012\n",
      "Epoch [50/100] Batch 0/1875                       Loss D: 1.1917, loss G: 1.0941\n",
      "Epoch [50/100] Batch 1874/1875                       Loss D: 1.2242, loss G: 1.1099\n",
      "Epoch [51/100] Batch 0/1875                       Loss D: 1.1573, loss G: 1.1727\n",
      "Epoch [51/100] Batch 1874/1875                       Loss D: 1.2458, loss G: 1.0772\n",
      "Epoch [52/100] Batch 0/1875                       Loss D: 0.9870, loss G: 1.2137\n",
      "Epoch [52/100] Batch 1874/1875                       Loss D: 1.2649, loss G: 1.0409\n",
      "Epoch [53/100] Batch 0/1875                       Loss D: 1.1289, loss G: 1.0594\n",
      "Epoch [53/100] Batch 1874/1875                       Loss D: 1.2832, loss G: 1.0093\n",
      "Epoch [54/100] Batch 0/1875                       Loss D: 1.5280, loss G: 0.9027\n",
      "Epoch [54/100] Batch 1874/1875                       Loss D: 1.2436, loss G: 1.0400\n",
      "Epoch [55/100] Batch 0/1875                       Loss D: 1.2200, loss G: 0.9007\n",
      "Epoch [55/100] Batch 1874/1875                       Loss D: 1.2274, loss G: 1.0620\n",
      "Epoch [56/100] Batch 0/1875                       Loss D: 1.1180, loss G: 0.9258\n",
      "Epoch [56/100] Batch 1874/1875                       Loss D: 1.2081, loss G: 1.0835\n",
      "Epoch [57/100] Batch 0/1875                       Loss D: 1.0443, loss G: 1.3567\n",
      "Epoch [57/100] Batch 1874/1875                       Loss D: 1.2041, loss G: 1.0884\n",
      "Epoch [58/100] Batch 0/1875                       Loss D: 1.2173, loss G: 1.0231\n",
      "Epoch [58/100] Batch 1874/1875                       Loss D: 1.2143, loss G: 1.0609\n",
      "Epoch [59/100] Batch 0/1875                       Loss D: 1.2193, loss G: 1.2522\n",
      "Epoch [59/100] Batch 1874/1875                       Loss D: 1.2057, loss G: 1.0946\n",
      "Epoch [60/100] Batch 0/1875                       Loss D: 1.4144, loss G: 1.0006\n",
      "Epoch [60/100] Batch 1874/1875                       Loss D: 1.2201, loss G: 1.0665\n",
      "Epoch [61/100] Batch 0/1875                       Loss D: 1.2842, loss G: 1.1403\n",
      "Epoch [61/100] Batch 1874/1875                       Loss D: 1.2621, loss G: 1.0200\n",
      "Epoch [62/100] Batch 0/1875                       Loss D: 1.4350, loss G: 0.7929\n",
      "Epoch [62/100] Batch 1874/1875                       Loss D: 1.2734, loss G: 0.9960\n",
      "Epoch [63/100] Batch 0/1875                       Loss D: 1.6823, loss G: 0.9214\n",
      "Epoch [63/100] Batch 1874/1875                       Loss D: 1.2673, loss G: 0.9874\n",
      "Epoch [64/100] Batch 0/1875                       Loss D: 1.2784, loss G: 0.9378\n",
      "Epoch [64/100] Batch 1874/1875                       Loss D: 1.2695, loss G: 0.9778\n",
      "Epoch [65/100] Batch 0/1875                       Loss D: 1.2367, loss G: 0.8750\n",
      "Epoch [65/100] Batch 1874/1875                       Loss D: 1.2524, loss G: 0.9862\n",
      "Epoch [66/100] Batch 0/1875                       Loss D: 1.5604, loss G: 0.5945\n",
      "Epoch [66/100] Batch 1874/1875                       Loss D: 1.2544, loss G: 0.9818\n",
      "Epoch [67/100] Batch 0/1875                       Loss D: 1.4216, loss G: 0.8925\n",
      "Epoch [67/100] Batch 1874/1875                       Loss D: 1.2653, loss G: 0.9710\n",
      "Epoch [68/100] Batch 0/1875                       Loss D: 1.1691, loss G: 0.8113\n",
      "Epoch [68/100] Batch 1874/1875                       Loss D: 1.2856, loss G: 0.9419\n",
      "Epoch [69/100] Batch 0/1875                       Loss D: 1.3636, loss G: 0.8773\n",
      "Epoch [69/100] Batch 1874/1875                       Loss D: 1.2834, loss G: 0.9389\n",
      "Epoch [70/100] Batch 0/1875                       Loss D: 1.1038, loss G: 1.0996\n",
      "Epoch [70/100] Batch 1874/1875                       Loss D: 1.2708, loss G: 0.9470\n",
      "Epoch [71/100] Batch 0/1875                       Loss D: 1.2751, loss G: 1.0474\n",
      "Epoch [71/100] Batch 1874/1875                       Loss D: 1.2621, loss G: 0.9527\n",
      "Epoch [72/100] Batch 0/1875                       Loss D: 1.2082, loss G: 0.9889\n",
      "Epoch [72/100] Batch 1874/1875                       Loss D: 1.2569, loss G: 0.9558\n",
      "Epoch [73/100] Batch 0/1875                       Loss D: 1.1635, loss G: 1.0690\n",
      "Epoch [73/100] Batch 1874/1875                       Loss D: 1.2690, loss G: 0.9373\n",
      "Epoch [74/100] Batch 0/1875                       Loss D: 1.2864, loss G: 1.0072\n",
      "Epoch [74/100] Batch 1874/1875                       Loss D: 1.2604, loss G: 0.9475\n",
      "Epoch [75/100] Batch 0/1875                       Loss D: 1.3747, loss G: 0.7810\n",
      "Epoch [75/100] Batch 1874/1875                       Loss D: 1.2656, loss G: 0.9459\n",
      "Epoch [76/100] Batch 0/1875                       Loss D: 1.3867, loss G: 0.8891\n",
      "Epoch [76/100] Batch 1874/1875                       Loss D: 1.2613, loss G: 0.9442\n",
      "Epoch [77/100] Batch 0/1875                       Loss D: 1.2413, loss G: 1.0863\n",
      "Epoch [77/100] Batch 1874/1875                       Loss D: 1.2550, loss G: 0.9432\n",
      "Epoch [78/100] Batch 0/1875                       Loss D: 1.1586, loss G: 1.1546\n",
      "Epoch [78/100] Batch 1874/1875                       Loss D: 1.2561, loss G: 0.9462\n",
      "Epoch [79/100] Batch 0/1875                       Loss D: 1.2114, loss G: 0.9865\n",
      "Epoch [79/100] Batch 1874/1875                       Loss D: 1.2598, loss G: 0.9478\n",
      "Epoch [80/100] Batch 0/1875                       Loss D: 1.2924, loss G: 0.9238\n",
      "Epoch [80/100] Batch 1874/1875                       Loss D: 1.2524, loss G: 0.9450\n",
      "Epoch [81/100] Batch 0/1875                       Loss D: 1.1873, loss G: 0.9506\n",
      "Epoch [81/100] Batch 1874/1875                       Loss D: 1.2495, loss G: 0.9550\n",
      "Epoch [82/100] Batch 0/1875                       Loss D: 1.3257, loss G: 1.0573\n",
      "Epoch [82/100] Batch 1874/1875                       Loss D: 1.2332, loss G: 0.9724\n",
      "Epoch [83/100] Batch 0/1875                       Loss D: 1.2150, loss G: 1.1750\n",
      "Epoch [83/100] Batch 1874/1875                       Loss D: 1.2349, loss G: 0.9731\n",
      "Epoch [84/100] Batch 0/1875                       Loss D: 1.2464, loss G: 1.0831\n",
      "Epoch [84/100] Batch 1874/1875                       Loss D: 1.2375, loss G: 0.9746\n",
      "Epoch [85/100] Batch 0/1875                       Loss D: 1.2631, loss G: 1.0665\n",
      "Epoch [85/100] Batch 1874/1875                       Loss D: 1.2321, loss G: 0.9775\n",
      "Epoch [86/100] Batch 0/1875                       Loss D: 1.1642, loss G: 1.0196\n",
      "Epoch [86/100] Batch 1874/1875                       Loss D: 1.2309, loss G: 0.9786\n",
      "Epoch [87/100] Batch 0/1875                       Loss D: 1.3090, loss G: 0.9141\n",
      "Epoch [87/100] Batch 1874/1875                       Loss D: 1.2305, loss G: 0.9788\n",
      "Epoch [88/100] Batch 0/1875                       Loss D: 1.4250, loss G: 0.8251\n",
      "Epoch [88/100] Batch 1874/1875                       Loss D: 1.2255, loss G: 0.9871\n",
      "Epoch [89/100] Batch 0/1875                       Loss D: 1.2500, loss G: 1.1182\n",
      "Epoch [89/100] Batch 1874/1875                       Loss D: 1.2293, loss G: 0.9835\n",
      "Epoch [90/100] Batch 0/1875                       Loss D: 1.2545, loss G: 0.9277\n",
      "Epoch [90/100] Batch 1874/1875                       Loss D: 1.2231, loss G: 0.9879\n",
      "Epoch [91/100] Batch 0/1875                       Loss D: 1.1749, loss G: 1.0143\n",
      "Epoch [91/100] Batch 1874/1875                       Loss D: 1.2193, loss G: 0.9966\n",
      "Epoch [92/100] Batch 0/1875                       Loss D: 1.0926, loss G: 1.1107\n",
      "Epoch [92/100] Batch 1874/1875                       Loss D: 1.2234, loss G: 0.9956\n",
      "Epoch [93/100] Batch 0/1875                       Loss D: 1.4347, loss G: 0.8119\n",
      "Epoch [93/100] Batch 1874/1875                       Loss D: 1.2201, loss G: 0.9951\n",
      "Epoch [94/100] Batch 0/1875                       Loss D: 1.2655, loss G: 1.1036\n",
      "Epoch [94/100] Batch 1874/1875                       Loss D: 1.2169, loss G: 1.0016\n",
      "Epoch [95/100] Batch 0/1875                       Loss D: 1.0229, loss G: 1.0416\n",
      "Epoch [95/100] Batch 1874/1875                       Loss D: 1.2192, loss G: 0.9988\n",
      "Epoch [96/100] Batch 0/1875                       Loss D: 1.2693, loss G: 0.8172\n",
      "Epoch [96/100] Batch 1874/1875                       Loss D: 1.2160, loss G: 0.9984\n",
      "Epoch [97/100] Batch 0/1875                       Loss D: 1.3802, loss G: 0.7474\n",
      "Epoch [97/100] Batch 1874/1875                       Loss D: 1.2125, loss G: 1.0077\n",
      "Epoch [98/100] Batch 0/1875                       Loss D: 1.2570, loss G: 0.9604\n",
      "Epoch [98/100] Batch 1874/1875                       Loss D: 1.2162, loss G: 1.0053\n",
      "Epoch [99/100] Batch 0/1875                       Loss D: 1.1086, loss G: 1.1165\n",
      "Epoch [99/100] Batch 1874/1875                       Loss D: 1.2147, loss G: 1.0123\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_gloss = 0\n",
    "    epoch_dloss = 0\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        if BCE:\n",
    "            lossD = (lossD_real + lossD_fake) / 2\n",
    "        else:\n",
    "            lossD = -(torch.mean(torch.log(disc_real) + torch.log(1. - disc_fake)))\n",
    "        #writer_real.add_scalar('d_loss', lossD, epoch)\n",
    "        epoch_dloss += lossD\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        if BCE:\n",
    "            lossG = criterion(output, torch.ones_like(output))\n",
    "        else:\n",
    "            lossG = -(torch.mean(torch.log(output)))\n",
    "        #writer_fake.add_scalar('g_loss', lossG, epoch)\n",
    "        epoch_gloss += lossG\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n",
    "    print(\n",
    "            f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {epoch_dloss/len(loader):.4f}, loss G: {epoch_gloss/len(loader):.4f}\"\n",
    "            )\n",
    "    writer_real.add_scalar('d_loss', epoch_dloss/len(loader), epoch)\n",
    "    writer_fake.add_scalar('g_loss', epoch_gloss/len(loader), epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
